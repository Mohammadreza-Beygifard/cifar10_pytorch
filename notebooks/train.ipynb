{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aaa443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e65ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from beepy import beep\n",
    "import torch.jit\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd71e45",
   "metadata": {},
   "source": [
    "## Use GPU if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23607dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device() -> torch.device:\n",
    "    \"\"\" Move the device to GPU if it is supported by the OS \"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4c32c",
   "metadata": {},
   "source": [
    "### Check if GPU is available\n",
    "\n",
    "For MacOS GPU is available through Pytorch MPS and for Windows and linux it is available through Pytorch Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = choose_device()\n",
    "print(device)\n",
    "x = torch.ones(1, device = device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a440b3",
   "metadata": {},
   "source": [
    "# Download and transform Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82edc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip_to_01(img):\n",
    "    # Clip image pixel values to the range [0, 1]\n",
    "    return torch.clamp(img, 0, 1)\n",
    "    \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    clip_to_01\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968e626",
   "metadata": {},
   "source": [
    "### Inspect the training data\n",
    "\n",
    "show_image() method gets an image and its label to let you inspect the training data.\n",
    "\n",
    "You can comment transforms.Normalize and clip_to_01 in the Download and transform Train data cell to inspect the real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image : torch.tensor, label : int) -> None:\n",
    "    assert image.size(0) == 3, \"First dimension should present the three color channels\"\n",
    "    assert image.size(1) == 32, \"Expected a 32 * 32 image\"\n",
    "    assert image.size(2) == 32, \"Expected a 32 * 32 image\"\n",
    "\n",
    "    red_channel = image[0]\n",
    "    green_channel = image[1]\n",
    "    blue_channel = image[2]\n",
    "    print(f\"label: {label}\")\n",
    "    plt.figure(figsize=(0.75,0.5))\n",
    "    rgb_image = np.stack([red_channel, green_channel, blue_channel], axis=2)\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_list = list(train_dataset)\n",
    "print(f\"train_dataset size: {len(train_dataset_list)}\")\n",
    "# Change this index to inspect diffrent images\n",
    "index = 0\n",
    "image, label = train_dataset_list[index]\n",
    "show_image(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83492001",
   "metadata": {},
   "source": [
    "## Define and train the Model\n",
    "\n",
    "The Model that I used in this notebook is a three block VGG feel free to play with it, by adding or removing blocks, changing the dropout and what ever that can help you to explore more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ebe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGThreeBlocks(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGGThreeBlocks, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Convolutional layers\n",
    "        # VGG 1\n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # VGG 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # VGG 3\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(0.85)\n",
    "        self.dropout2 = nn.Dropout(0.85)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn = nn.BatchNorm1d(128 * 4 * 4)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 64 channels, 4x4 image size after pooling\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.predicator = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # VGG 1\n",
    "        x = self.relu0(self.conv0(x))\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        # VGG 2\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        # VGG 3\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool5(self.relu5(self.conv5(x)))\n",
    "        \n",
    "        # Flatten and batch normalize the tensor for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout1(self.relu4(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu5(self.fc2(x)))\n",
    "        x = self.predicator(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def custom_weight_init(module):\n",
    "    \"\"\" You can initialize your model weights by a custom method \"\"\"\n",
    "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "        # Initialize weights using your custom logic\n",
    "        nn.init.xavier_normal_(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41d7e8",
   "metadata": {},
   "source": [
    "### A Method for drawing the loss function\n",
    "\n",
    "Drawing the loss function against the epochs can help you to get a feeling on how you are training your model and if you need to change some hyper parameter to train the model more effective.\n",
    "\n",
    "For example, if your loss function is not reducing \"almost\" monotonically, probably your learning rate is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b607ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss : list, epochs : list):\n",
    "     plt.title('Cross Entropy Loss')\n",
    "     plt.plot(epochs, loss, color='blue', label='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54b784",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "Here we are creating the model and applying the custom weight initialization. Feel free to use a different initialization technique or just comment the model.apply() method to go on with the torch default weight initialization.\n",
    "\n",
    "**state_dict_epoch** is used to save the sate of the model when the loss function is in its lowest value. This technique is called early drop.\n",
    "\n",
    "**number of epochs** is also defined in this cell. Change it based on your need, for example run the model for a single epoch to check if everything is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create or reset the model\n",
    "model = VGGThreeBlocks(num_classes=10)\n",
    "model.apply(custom_weight_init)\n",
    "\n",
    "state_dict_epoch = {\n",
    "    \"epoch\": 0,\n",
    "    \"loss\": 0,\n",
    "    \"state_dict\":{}\n",
    "}\n",
    "\n",
    "# This is used for drawing the loss against the epochs\n",
    "loss_list = []\n",
    "\n",
    "# Use a single or low number of epochs for debuging purposes\n",
    "num_epochs = 100\n",
    "epochs = range(num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b42030",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "It is worth to mention that I use ExponentialLR scheduler to reduce the learning rate after each epoch to reduce the learning rate and avoid an unstable loss function behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e192fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device) #Use GPU if it is available, check the first cell for more info\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay = 0.001)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_list.append(loss.item())\n",
    "        \n",
    "    if(epoch == 0):   \n",
    "        state_dict_epoch[\"epoch\"] = epoch\n",
    "        state_dict_epoch[\"loss\"] = loss.item()\n",
    "        state_dict_epoch[\"state_dict\"] = model.state_dict()\n",
    "    if(epoch > 1 and loss.item() < state_dict_epoch[\"loss\"]):\n",
    "        state_dict_epoch[\"epoch\"] = epoch\n",
    "        state_dict_epoch[\"loss\"] = loss.item()\n",
    "        state_dict_epoch[\"state_dict\"] = model.state_dict()\n",
    "    \n",
    "    #scheduler.step()\n",
    "    if((epoch +1)%(num_epochs / 5) == 0 or epoch == 0):    \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\"\"Minimum loss has happened at epoch number {state_dict_epoch[\"epoch\"]}\"\"\")\n",
    "for _ in range(1):\n",
    "    beep(sound = \"success\")\n",
    "    \n",
    "plot_loss(loss_list, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24986f02",
   "metadata": {},
   "source": [
    "## Save the trained model state\n",
    "\n",
    "Here I am dumping the model state in its lowest loss.\n",
    "\n",
    "feel free to change the first parameter with model.state_dict() to drop the model state at its latest state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ff6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"./model_state\")\n",
    "torch.save(state_dict_epoch[\"state_dict\"], \"./model_state/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40933711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model_state/model.pt\"))\n",
    "model.eval() # To avoid dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1431b",
   "metadata": {},
   "source": [
    "# Evaluate the trained model\n",
    "\n",
    "Here we load the downloaded test data and evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "validation_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe53996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.device(\"cpu\"))\n",
    "model.eval() # To avoid dropout\n",
    "correct = 0\n",
    "total = 0\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, labels in validation_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        current_accuracy = 100 * correct / total\n",
    "        accuracy_list.append(current_accuracy)\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
